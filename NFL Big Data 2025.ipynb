{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59afb4c0-e894-4e42-8cf9-f52ba1e1a853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\musar\\anaconda3\\lib\\site-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (2.2.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\musar\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\musar\\anaconda3\\lib\\site-packages (from bleach->kaggle) (23.2)\n",
      "Requirement already satisfied: webencodings in c:\\users\\musar\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\musar\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\musar\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\musar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mysql-connector-python in c:\\users\\musar\\anaconda3\\lib\\site-packages (9.1.0)\n",
      "nfl-big-data-bowl-2025.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "!pip install kaggle\n",
    "!pip install pandas\n",
    "!pip install mysql-connector-python\n",
    "import kaggle\n",
    "!kaggle competitions download nfl-big-data-bowl-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4898d381-3c62-4cd4-b319-71574a61b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfl-big-data-bowl-2025.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "#downlaod dataset using kaggle api \n",
    "!kaggle competitions download -c nfl-big-data-bowl-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c11c52-34d8-4c79-9289-f7fd9ddcafcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract file from zip file \n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the zip file name and extraction directory\n",
    "zip_file = 'nfl-big-data-bowl-2025.zip'\n",
    "extract_dir = 'nfl_data'\n",
    "\n",
    "# Extract all the files\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir) # extract file to directory\n",
    "zip_ref.close() # close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d03e35-0c4d-4d4b-9809-373b18cda281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['games.csv', 'players.csv', 'player_play.csv', 'plays.csv', 'tracking_week_1.csv', 'tracking_week_2.csv', 'tracking_week_3.csv', 'tracking_week_4.csv', 'tracking_week_5.csv', 'tracking_week_6.csv', 'tracking_week_7.csv', 'tracking_week_8.csv', 'tracking_week_9.csv']\n"
     ]
    }
   ],
   "source": [
    "#verify .cvs file in zip folder\n",
    "import os\n",
    "\n",
    "# List all files in the extracted directory\n",
    "extracted_files = os.listdir(extract_dir)\n",
    "print(extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721ab9a5-13d2-441f-8bb6-b1b120606ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from games.csv:\n",
      "       gameId  season  week  gameDate gameTimeEastern homeTeamAbbr  \\\n",
      "0  2022090800    2022     1  9/8/2022        20:20:00           LA   \n",
      "\n",
      "  visitorTeamAbbr  homeFinalScore  visitorFinalScore  \n",
      "0             BUF              10                 31  \n"
     ]
    }
   ],
   "source": [
    "#read data from the file and handle null values \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory where the files are extracted\n",
    "extract_dir = 'nfl_data'\n",
    "\n",
    "# List of CSV files (you can modify this based on your actual files)\n",
    "csv_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Read all CSV files into a dictionary of DataFrames\n",
    "dfs = {file: pd.read_csv(os.path.join(extract_dir, file)) for file in csv_files}\n",
    "\n",
    "# Access specific DataFrames\n",
    "#for file_name, df in dfs.items():\n",
    "print(f\"Data from {'games.csv'}:\")\n",
    "    \n",
    "# Specify the full path to the games.csv file\n",
    "df = pd.read_csv(os.path.join(extract_dir, 'games.csv'))\n",
    "print(df.head(1))  # Display first few rows of each DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a37360e-820a-4527-b61c-d451442ffaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       gameid  season  week   gamedate gametimeeastern hometeamabbr  \\\n",
      "0  2022090800    2022     1   9/8/2022        20:20:00           LA   \n",
      "1  2022091100    2022     1  9/11/2022        13:00:00          ATL   \n",
      "2  2022091101    2022     1  9/11/2022        13:00:00          CAR   \n",
      "3  2022091102    2022     1  9/11/2022        13:00:00          CHI   \n",
      "4  2022091103    2022     1  9/11/2022        13:00:00          CIN   \n",
      "5  2022091104    2022     1  9/11/2022        13:00:00          DET   \n",
      "6  2022091105    2022     1  9/11/2022        13:00:00          HOU   \n",
      "7  2022091106    2022     1  9/11/2022        13:00:00          MIA   \n",
      "8  2022091107    2022     1  9/11/2022        13:00:00          NYJ   \n",
      "9  2022091109    2022     1  9/11/2022        13:00:00          WAS   \n",
      "\n",
      "  visitorteamabbr  homefinalscore  visitorfinalscore  \n",
      "0             BUF              10                 31  \n",
      "1              NO              26                 27  \n",
      "2             CLE              24                 26  \n",
      "3              SF              19                 10  \n",
      "4             PIT              20                 23  \n",
      "5             PHI              35                 38  \n",
      "6             IND              20                 20  \n",
      "7              NE              20                  7  \n",
      "8             BAL               9                 24  \n",
      "9             JAX              28                 22  \n"
     ]
    }
   ],
   "source": [
    "#rename column names- make them lower case and replace space or separate words with underscore\n",
    "df.columns=df.columns.str.lower()\n",
    "df.columns=df.columns.str.replace(' ','_')\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "883437c6-fbbd-4960-8622-fb853bbe41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data into sql server using replace option\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Define the append_to_sql function\n",
    "def append_to_sql(df, table_name, connection):\n",
    "    \"\"\"\n",
    "    Appends data from a DataFrame to a MySQL table.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to append.\n",
    "        table_name (str): The name of the target table in the database.\n",
    "        connection (mysql.connector.connection_cext.CMySQLConnection): The MySQL connection object.\n",
    "    \"\"\"\n",
    "    # Generate column placeholders dynamically based on DataFrame columns\n",
    "    columns = ', '.join(df.columns)\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    \n",
    "    # Construct the INSERT query\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "    \n",
    "    # Convert DataFrame to list of tuples for bulk insertion\n",
    "    values = [tuple(row) for row in df.to_numpy()]\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor = connection.cursor()\n",
    "    cursor.executemany(insert_query, values)\n",
    "    connection.commit()\n",
    "    print(f\"Data appended successfully to {table_name}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f9207-2468-42fa-aabe-0a0af2223bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'id': [3, 4], 'name': ['Charlie', 'David']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Establish MySQL connection\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='Asc90937866!',\n",
    "    database='MySQLMusa'\n",
    ")\n",
    "\n",
    "# Call the function to append data\n",
    "append_to_sql(df, 'df_orders', conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
